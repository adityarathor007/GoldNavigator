{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058b4754-ca61-4961-af7d-3401be884604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 01:57:41.005645: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-18 01:57:41.340447: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 01:57:42.234713: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-18 01:57:42.234812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-18 01:57:42.234820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53328683-fec7-4269-9693-d43b4aea21fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1718\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SP_open</th>\n",
       "      <th>SP_high</th>\n",
       "      <th>SP_low</th>\n",
       "      <th>...</th>\n",
       "      <th>GDX_Low</th>\n",
       "      <th>GDX_Close</th>\n",
       "      <th>GDX_Adj Close</th>\n",
       "      <th>GDX_Volume</th>\n",
       "      <th>USO_Open</th>\n",
       "      <th>USO_High</th>\n",
       "      <th>USO_Low</th>\n",
       "      <th>USO_Close</th>\n",
       "      <th>USO_Adj Close</th>\n",
       "      <th>USO_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-15</td>\n",
       "      <td>154.740005</td>\n",
       "      <td>154.949997</td>\n",
       "      <td>151.710007</td>\n",
       "      <td>152.330002</td>\n",
       "      <td>152.330002</td>\n",
       "      <td>21521900</td>\n",
       "      <td>123.029999</td>\n",
       "      <td>123.199997</td>\n",
       "      <td>121.989998</td>\n",
       "      <td>...</td>\n",
       "      <td>51.570000</td>\n",
       "      <td>51.680000</td>\n",
       "      <td>48.973877</td>\n",
       "      <td>20605600</td>\n",
       "      <td>36.900002</td>\n",
       "      <td>36.939999</td>\n",
       "      <td>36.049999</td>\n",
       "      <td>36.130001</td>\n",
       "      <td>36.130001</td>\n",
       "      <td>12616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-12-16</td>\n",
       "      <td>154.309998</td>\n",
       "      <td>155.369995</td>\n",
       "      <td>153.899994</td>\n",
       "      <td>155.229996</td>\n",
       "      <td>155.229996</td>\n",
       "      <td>18124300</td>\n",
       "      <td>122.230003</td>\n",
       "      <td>122.949997</td>\n",
       "      <td>121.300003</td>\n",
       "      <td>...</td>\n",
       "      <td>52.040001</td>\n",
       "      <td>52.680000</td>\n",
       "      <td>49.921513</td>\n",
       "      <td>16285400</td>\n",
       "      <td>36.180000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>35.730000</td>\n",
       "      <td>36.270000</td>\n",
       "      <td>36.270000</td>\n",
       "      <td>12578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-12-19</td>\n",
       "      <td>155.479996</td>\n",
       "      <td>155.860001</td>\n",
       "      <td>154.360001</td>\n",
       "      <td>154.869995</td>\n",
       "      <td>154.869995</td>\n",
       "      <td>12547200</td>\n",
       "      <td>122.059998</td>\n",
       "      <td>122.320000</td>\n",
       "      <td>120.029999</td>\n",
       "      <td>...</td>\n",
       "      <td>51.029999</td>\n",
       "      <td>51.169998</td>\n",
       "      <td>48.490578</td>\n",
       "      <td>15120200</td>\n",
       "      <td>36.389999</td>\n",
       "      <td>36.450001</td>\n",
       "      <td>35.930000</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>7418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>156.820007</td>\n",
       "      <td>157.429993</td>\n",
       "      <td>156.580002</td>\n",
       "      <td>156.979996</td>\n",
       "      <td>156.979996</td>\n",
       "      <td>9136300</td>\n",
       "      <td>122.180000</td>\n",
       "      <td>124.139999</td>\n",
       "      <td>120.370003</td>\n",
       "      <td>...</td>\n",
       "      <td>52.369999</td>\n",
       "      <td>52.990002</td>\n",
       "      <td>50.215282</td>\n",
       "      <td>11644900</td>\n",
       "      <td>37.299999</td>\n",
       "      <td>37.610001</td>\n",
       "      <td>37.220001</td>\n",
       "      <td>37.560001</td>\n",
       "      <td>37.560001</td>\n",
       "      <td>10041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-12-21</td>\n",
       "      <td>156.979996</td>\n",
       "      <td>157.529999</td>\n",
       "      <td>156.130005</td>\n",
       "      <td>157.160004</td>\n",
       "      <td>157.160004</td>\n",
       "      <td>11996100</td>\n",
       "      <td>123.930000</td>\n",
       "      <td>124.360001</td>\n",
       "      <td>122.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.419998</td>\n",
       "      <td>52.959999</td>\n",
       "      <td>50.186852</td>\n",
       "      <td>8724300</td>\n",
       "      <td>37.669998</td>\n",
       "      <td>38.240002</td>\n",
       "      <td>37.520000</td>\n",
       "      <td>38.110001</td>\n",
       "      <td>38.110001</td>\n",
       "      <td>10728000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        Open        High         Low       Close   Adj Close  \\\n",
       "0 2011-12-15  154.740005  154.949997  151.710007  152.330002  152.330002   \n",
       "1 2011-12-16  154.309998  155.369995  153.899994  155.229996  155.229996   \n",
       "2 2011-12-19  155.479996  155.860001  154.360001  154.869995  154.869995   \n",
       "3 2011-12-20  156.820007  157.429993  156.580002  156.979996  156.979996   \n",
       "4 2011-12-21  156.979996  157.529999  156.130005  157.160004  157.160004   \n",
       "\n",
       "     Volume     SP_open     SP_high      SP_low  ...    GDX_Low  GDX_Close  \\\n",
       "0  21521900  123.029999  123.199997  121.989998  ...  51.570000  51.680000   \n",
       "1  18124300  122.230003  122.949997  121.300003  ...  52.040001  52.680000   \n",
       "2  12547200  122.059998  122.320000  120.029999  ...  51.029999  51.169998   \n",
       "3   9136300  122.180000  124.139999  120.370003  ...  52.369999  52.990002   \n",
       "4  11996100  123.930000  124.360001  122.750000  ...  52.419998  52.959999   \n",
       "\n",
       "   GDX_Adj Close  GDX_Volume   USO_Open   USO_High    USO_Low  USO_Close  \\\n",
       "0      48.973877    20605600  36.900002  36.939999  36.049999  36.130001   \n",
       "1      49.921513    16285400  36.180000  36.500000  35.730000  36.270000   \n",
       "2      48.490578    15120200  36.389999  36.450001  35.930000  36.200001   \n",
       "3      50.215282    11644900  37.299999  37.610001  37.220001  37.560001   \n",
       "4      50.186852     8724300  37.669998  38.240002  37.520000  38.110001   \n",
       "\n",
       "   USO_Adj Close  USO_Volume  \n",
       "0      36.130001    12616700  \n",
       "1      36.270000    12578800  \n",
       "2      36.200001     7418200  \n",
       "3      37.560001    10041600  \n",
       "4      38.110001    10728000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"raw_gold_price.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700dfb61-5a41-4903-90ee-27d9b173f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>GDX_Close</th>\n",
       "      <th>SF_Price</th>\n",
       "      <th>EG_close</th>\n",
       "      <th>PLT_Price</th>\n",
       "      <th>USDI_Price</th>\n",
       "      <th>OF_Price</th>\n",
       "      <th>SP_close</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>157.779999</td>\n",
       "      <td>53.900002</td>\n",
       "      <td>52740.0</td>\n",
       "      <td>73.199997</td>\n",
       "      <td>1421.25</td>\n",
       "      <td>81.250</td>\n",
       "      <td>112.74</td>\n",
       "      <td>128.039993</td>\n",
       "      <td>Wall Street Experts: UBS Analysts Love These 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>159.669998</td>\n",
       "      <td>54.310001</td>\n",
       "      <td>52607.0</td>\n",
       "      <td>71.699997</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>81.614</td>\n",
       "      <td>112.24</td>\n",
       "      <td>129.199997</td>\n",
       "      <td>A. Schulman Enters Oversold Territory - Tale o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>160.380005</td>\n",
       "      <td>54.720001</td>\n",
       "      <td>52770.0</td>\n",
       "      <td>70.650002</td>\n",
       "      <td>1498.95</td>\n",
       "      <td>81.036</td>\n",
       "      <td>111.26</td>\n",
       "      <td>129.509995</td>\n",
       "      <td>Zacks Releases Four Powerful ''Buy'' Stocks: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>162.009995</td>\n",
       "      <td>51.820000</td>\n",
       "      <td>55365.0</td>\n",
       "      <td>66.599998</td>\n",
       "      <td>1550.75</td>\n",
       "      <td>80.026</td>\n",
       "      <td>110.03</td>\n",
       "      <td>131.460007</td>\n",
       "      <td>Waters Outperforms Q4 Estimate - Analyst Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>167.350006</td>\n",
       "      <td>54.150002</td>\n",
       "      <td>55926.0</td>\n",
       "      <td>67.750000</td>\n",
       "      <td>1635.60</td>\n",
       "      <td>79.456</td>\n",
       "      <td>119.58</td>\n",
       "      <td>136.410004</td>\n",
       "      <td>Agilent Reports In Line - Analyst Blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Close  GDX_Close  SF_Price   EG_close  PLT_Price  \\\n",
       "30 2012-01-05  157.779999  53.900002   52740.0  73.199997    1421.25   \n",
       "34 2012-01-11  159.669998  54.310001   52607.0  71.699997    1499.55   \n",
       "35 2012-01-12  160.380005  54.720001   52770.0  70.650002    1498.95   \n",
       "42 2012-01-24  162.009995  51.820000   55365.0  66.599998    1550.75   \n",
       "59 2012-02-17  167.350006  54.150002   55926.0  67.750000    1635.60   \n",
       "\n",
       "    USDI_Price  OF_Price    SP_close  \\\n",
       "30      81.250    112.74  128.039993   \n",
       "34      81.614    112.24  129.199997   \n",
       "35      81.036    111.26  129.509995   \n",
       "42      80.026    110.03  131.460007   \n",
       "59      79.456    119.58  136.410004   \n",
       "\n",
       "                                             Headline  \n",
       "30  Wall Street Experts: UBS Analysts Love These 1...  \n",
       "34  A. Schulman Enters Oversold Territory - Tale o...  \n",
       "35  Zacks Releases Four Powerful ''Buy'' Stocks: I...  \n",
       "42      Waters Outperforms Q4 Estimate - Analyst Blog  \n",
       "59             Agilent Reports In Line - Analyst Blog  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv(\"news.csv\")\n",
    "news_df['Date'] = pd.to_datetime(news_df['Date'], format='%b %d, %Y')\n",
    "news_df = news_df.groupby('Date')['Headline'].apply(' '.join).reset_index()\n",
    "merged_df = pd.merge(df[['Date','Close','GDX_Close','SF_Price', 'EG_close', 'PLT_Price','USDI_Price','OF_Price', 'SP_close']], news_df, on='Date', how='outer')\n",
    "merged_df = merged_df[merged_df['Date'] >= '2011-12-12']\n",
    "combined_headline = ''\n",
    "for index, row in merged_df.iterrows():\n",
    "    if pd.isna(row['Close']):\n",
    "        combined_headline+=row['Headline']\n",
    "    else:\n",
    "        if type(row['Headline'])==str:\n",
    "            combined_headline+=row['Headline']\n",
    "        merged_df.at[index,'Headline'] = combined_headline\n",
    "        combined_headline=''\n",
    "merged_df = merged_df.dropna(subset=['Close'])\n",
    "merged_df = merged_df[merged_df['Headline'] != '']\n",
    "print(len(merged_df))\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e957b2c-ce78-4be6-9b7f-89ca51462d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>MAP_3</th>\n",
       "      <th>MAP_9</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>157.779999</td>\n",
       "      <td>156.803335</td>\n",
       "      <td>153.959445</td>\n",
       "      <td>Wall Street Experts: UBS Analysts Love These 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>159.669998</td>\n",
       "      <td>158.269999</td>\n",
       "      <td>157.346666</td>\n",
       "      <td>A. Schulman Enters Oversold Territory - Tale o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>160.380005</td>\n",
       "      <td>159.563334</td>\n",
       "      <td>157.842222</td>\n",
       "      <td>Zacks Releases Four Powerful ''Buy'' Stocks: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>162.009995</td>\n",
       "      <td>162.655557</td>\n",
       "      <td>161.775558</td>\n",
       "      <td>Waters Outperforms Q4 Estimate - Analyst Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>167.350006</td>\n",
       "      <td>167.820002</td>\n",
       "      <td>167.544444</td>\n",
       "      <td>Agilent Reports In Line - Analyst Blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       Close       MAP_3       MAP_9  \\\n",
       "0 2012-01-05  157.779999  156.803335  153.959445   \n",
       "1 2012-01-11  159.669998  158.269999  157.346666   \n",
       "2 2012-01-12  160.380005  159.563334  157.842222   \n",
       "3 2012-01-24  162.009995  162.655557  161.775558   \n",
       "4 2012-02-17  167.350006  167.820002  167.544444   \n",
       "\n",
       "                                            Headline  \n",
       "0  Wall Street Experts: UBS Analysts Love These 1...  \n",
       "1  A. Schulman Enters Oversold Territory - Tale o...  \n",
       "2  Zacks Releases Four Powerful ''Buy'' Stocks: I...  \n",
       "3      Waters Outperforms Q4 Estimate - Analyst Blog  \n",
       "4             Agilent Reports In Line - Analyst Blog  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_df = pd.read_csv(\"processed_gold_price.csv\")[['Date','Close']]\n",
    "map_df['Date'] = pd.to_datetime(map_df['Date'], format='%Y-%m-%d')\n",
    "map_df['MAP_3'] = map_df['Close'].rolling(window=3).mean()\n",
    "map_df['MAP_9'] = map_df['Close'].rolling(window=9).mean()\n",
    "map_df.dropna(inplace=True)\n",
    "map_df = map_df.merge(merged_df[['Date','Headline']], on='Date',how='inner')\n",
    "print(len(map_df))\n",
    "map_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b8e92-0d9a-4e95-be78-ce1022a2fbe8",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9718c5-4e9f-4c83-878d-6fd6524a8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.41218101634604537\n",
      "R^2 Score: 0.9980167281730175\n",
      "Percentage error: 1.2212318890938203\n"
     ]
    }
   ],
   "source": [
    "X = map_df.drop(columns=['Headline','Close','Date'])\n",
    "Y = map_df['Close']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)\n",
    "print(\"Percentage error:\", np.mean(np.abs(Y_test-Y_pred/Y_test))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ef292-b4b1-4406-ae59-ea182020f280",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c424551-8496-43ee-bab4-c10e405ceb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 01:57:46.552990: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-11-18 01:57:46.553023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: aditechbuddy-IdeaPad-Gaming-3-15ACH6\n",
      "2024-11-18 01:57:46.553032: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: aditechbuddy-IdeaPad-Gaming-3-15ACH6\n",
      "2024-11-18 01:57:46.553135: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 535.183.1\n",
      "2024-11-18 01:57:46.553156: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 535.183.1\n",
      "2024-11-18 01:57:46.553161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 535.183.1\n",
      "2024-11-18 01:57:46.553395: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "25/25 [==============================] - 7s 226ms/step - loss: 14768.7100 - mse: 14768.7100 - val_loss: 13433.9844 - val_mse: 13433.9844\n",
      "Epoch 2/35\n",
      "25/25 [==============================] - 5s 208ms/step - loss: 11982.2236 - mse: 11982.2236 - val_loss: 10297.8057 - val_mse: 10297.8057\n",
      "Epoch 3/35\n",
      "25/25 [==============================] - 5s 205ms/step - loss: 8835.1201 - mse: 8835.1201 - val_loss: 7194.7964 - val_mse: 7194.7964\n",
      "Epoch 4/35\n",
      "25/25 [==============================] - 5s 202ms/step - loss: 5891.6265 - mse: 5891.6265 - val_loss: 4378.9702 - val_mse: 4378.9702\n",
      "Epoch 5/35\n",
      "25/25 [==============================] - 5s 202ms/step - loss: 3378.5488 - mse: 3378.5488 - val_loss: 2177.5676 - val_mse: 2177.5676\n",
      "Epoch 6/35\n",
      "25/25 [==============================] - 5s 209ms/step - loss: 1626.5314 - mse: 1626.5314 - val_loss: 822.4566 - val_mse: 822.4566\n",
      "Epoch 7/35\n",
      "25/25 [==============================] - 5s 210ms/step - loss: 668.4285 - mse: 668.4285 - val_loss: 296.3397 - val_mse: 296.3397\n",
      "Epoch 8/35\n",
      "25/25 [==============================] - 5s 211ms/step - loss: 408.6928 - mse: 408.6928 - val_loss: 166.6583 - val_mse: 166.6583\n",
      "Epoch 9/35\n",
      "25/25 [==============================] - 5s 194ms/step - loss: 376.0756 - mse: 376.0756 - val_loss: 132.2150 - val_mse: 132.2150\n",
      "Epoch 10/35\n",
      "25/25 [==============================] - 5s 189ms/step - loss: 388.0454 - mse: 388.0454 - val_loss: 117.8751 - val_mse: 117.8751\n",
      "Epoch 11/35\n",
      "25/25 [==============================] - 5s 199ms/step - loss: 325.4638 - mse: 325.4638 - val_loss: 107.6746 - val_mse: 107.6746\n",
      "Epoch 12/35\n",
      "25/25 [==============================] - 5s 204ms/step - loss: 317.9310 - mse: 317.9310 - val_loss: 97.2942 - val_mse: 97.2942\n",
      "Epoch 13/35\n",
      "25/25 [==============================] - 5s 192ms/step - loss: 330.4264 - mse: 330.4264 - val_loss: 83.3612 - val_mse: 83.3612\n",
      "Epoch 14/35\n",
      "25/25 [==============================] - 5s 193ms/step - loss: 326.6894 - mse: 326.6894 - val_loss: 73.5637 - val_mse: 73.5637\n",
      "Epoch 15/35\n",
      "25/25 [==============================] - 5s 208ms/step - loss: 302.6886 - mse: 302.6886 - val_loss: 66.2158 - val_mse: 66.2158\n",
      "Epoch 16/35\n",
      "25/25 [==============================] - 5s 215ms/step - loss: 294.9296 - mse: 294.9296 - val_loss: 59.8308 - val_mse: 59.8308\n",
      "Epoch 17/35\n",
      "25/25 [==============================] - 5s 214ms/step - loss: 278.7075 - mse: 278.7075 - val_loss: 52.9708 - val_mse: 52.9708\n",
      "Epoch 18/35\n",
      "25/25 [==============================] - 5s 194ms/step - loss: 288.6765 - mse: 288.6765 - val_loss: 45.7274 - val_mse: 45.7274\n",
      "Epoch 19/35\n",
      "25/25 [==============================] - 5s 196ms/step - loss: 311.3995 - mse: 311.3995 - val_loss: 42.9600 - val_mse: 42.9600\n",
      "Epoch 20/35\n",
      "25/25 [==============================] - 5s 201ms/step - loss: 269.6924 - mse: 269.6924 - val_loss: 39.0595 - val_mse: 39.0595\n",
      "Epoch 21/35\n",
      "25/25 [==============================] - 5s 196ms/step - loss: 255.5045 - mse: 255.5045 - val_loss: 39.4138 - val_mse: 39.4138\n",
      "Epoch 22/35\n",
      "25/25 [==============================] - 5s 195ms/step - loss: 272.5471 - mse: 272.5471 - val_loss: 37.4419 - val_mse: 37.4419\n",
      "Epoch 23/35\n",
      "25/25 [==============================] - 5s 204ms/step - loss: 293.9782 - mse: 293.9782 - val_loss: 33.3871 - val_mse: 33.3871\n",
      "Epoch 24/35\n",
      "25/25 [==============================] - 5s 198ms/step - loss: 300.0980 - mse: 300.0980 - val_loss: 31.2757 - val_mse: 31.2757\n",
      "Epoch 25/35\n",
      "25/25 [==============================] - 5s 195ms/step - loss: 280.9933 - mse: 280.9933 - val_loss: 30.5174 - val_mse: 30.5174\n",
      "Epoch 26/35\n",
      "25/25 [==============================] - 5s 200ms/step - loss: 288.5428 - mse: 288.5428 - val_loss: 31.5199 - val_mse: 31.5199\n",
      "Epoch 27/35\n",
      "25/25 [==============================] - 5s 196ms/step - loss: 268.2216 - mse: 268.2216 - val_loss: 28.7629 - val_mse: 28.7629\n",
      "Epoch 28/35\n",
      "25/25 [==============================] - 5s 191ms/step - loss: 291.3174 - mse: 291.3174 - val_loss: 26.3894 - val_mse: 26.3894\n",
      "Epoch 29/35\n",
      "25/25 [==============================] - 5s 207ms/step - loss: 282.7458 - mse: 282.7458 - val_loss: 26.1534 - val_mse: 26.1534\n",
      "Epoch 30/35\n",
      "25/25 [==============================] - 5s 200ms/step - loss: 227.7867 - mse: 227.7867 - val_loss: 26.9032 - val_mse: 26.9032\n",
      "Epoch 31/35\n",
      "25/25 [==============================] - 5s 201ms/step - loss: 268.9717 - mse: 268.9717 - val_loss: 29.8014 - val_mse: 29.8014\n",
      "Epoch 32/35\n",
      "25/25 [==============================] - 5s 204ms/step - loss: 259.9641 - mse: 259.9641 - val_loss: 23.3910 - val_mse: 23.3910\n",
      "Epoch 33/35\n",
      "25/25 [==============================] - 5s 193ms/step - loss: 293.4750 - mse: 293.4750 - val_loss: 26.6510 - val_mse: 26.6510\n",
      "Epoch 34/35\n",
      "25/25 [==============================] - 5s 197ms/step - loss: 272.7862 - mse: 272.7862 - val_loss: 23.3915 - val_mse: 23.3915\n",
      "Epoch 35/35\n",
      "25/25 [==============================] - 5s 196ms/step - loss: 263.9387 - mse: 263.9387 - val_loss: 21.5234 - val_mse: 21.5234\n",
      "3/3 [==============================] - 0s 63ms/step\n",
      "Mean Squared Error on Test Set: 14.8025\n",
      "Mean Squared Error: 14.80248551992174\n",
      "R^2 Score: 0.9277392421778368\n",
      "Percentage error: 1.2166519316037496\n",
      "Actual Price: [124.099998 107.839996 122.239998 126.190002 115.120003]\n",
      "Predicted_Price: [[121.97521 ]\n",
      " [115.035576]\n",
      " [119.66906 ]\n",
      " [124.427155]\n",
      " [115.14644 ]]\n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "Actual Price: [164.34666433333334 165.58667 165.678889 165.74000033333334\n",
      " 163.61889122222223 160.58333333333334 160.52333066666665 160.699997\n",
      " 161.52666699999997 162.32166533333333]\n",
      "Predicted Price:161.44934 162.11801 162.16196 162.18895 160.60223 158.06161 157.62057 157.47339 158.08646 158.79037 "
     ]
    }
   ],
   "source": [
    "numerical_features = map_df[map_df.columns[2:4]].values\n",
    "scaler = StandardScaler()\n",
    "numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Preprocessing Text Features for LSTM\n",
    "MAX_WORDS = 10000  # Maximum number of words in vocabulary\n",
    "MAX_SEQ_LENGTH = 1000  # Maximum sequence length for padding\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(map_df['Headline'])\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "text_sequences = tokenizer.texts_to_sequences(map_df['Headline'])\n",
    "text_features = pad_sequences(text_sequences, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Labels\n",
    "labels = map_df['Close'].values\n",
    "\n",
    "# Train-Test Split\n",
    "X_train_num, X_test_num, X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    numerical_features_scaled, text_features, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Define Model\n",
    "# Numerical Input\n",
    "numerical_input = Input(shape=(2,), name=\"numerical_input\")\n",
    "X_num = Dense(8, activation=\"relu\")(numerical_input)\n",
    "X_num = Dropout(0.3)(X_num)\n",
    "\n",
    "# Text Input with LSTM\n",
    "text_input = Input(shape=(MAX_SEQ_LENGTH,), name=\"text_input\")\n",
    "embedding = Embedding(input_dim=MAX_WORDS, output_dim=32)(text_input)\n",
    "X_text = LSTM(80, return_sequences=False)(embedding)\n",
    "X_text = Dropout(0.2)(X_text)\n",
    "\n",
    "# Combine Numerical and Text Branches\n",
    "combined = concatenate([X_num, X_text])\n",
    "X = Dense(200, activation=\"relu\")(combined)\n",
    "X = Dense(50, activation=\"relu\")(combined)\n",
    "X = Dropout(0.2)(X)\n",
    "output = Dense(1, activation=\"linear\")(X)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=[numerical_input, text_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_train_num, X_train_text],\n",
    "    Y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=35,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "Y_pred = model.predict([X_test_num, X_test_text])\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)\n",
    "print(\"Percentage error:\", np.mean(np.abs(Y_pred-Y_test/Y_test))/100)\n",
    "print(\"Actual Price:\",Y_test[:5])\n",
    "print(\"Predicted_Price:\",Y_pred[:5])\n",
    "\n",
    "#Make new prediction\n",
    "row = np.array(map_df)[50:60,1:]\n",
    "X_num = row[:,1:3]\n",
    "X_text = row[:,3]\n",
    "Y = row[:,1]\n",
    "\n",
    "new_numerical_features_scaled = scaler.transform(X_num)\n",
    "new_text_sequence = tokenizer.texts_to_sequences(X_text)\n",
    "new_text_features = pad_sequences(new_text_sequence, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "predicted_close_price = model.predict([new_numerical_features_scaled, new_text_features])\n",
    "\n",
    "print(\"Actual Price:\", Y)\n",
    "print(\"Predicted Price:\",end='')\n",
    "for i, prediction in enumerate(predicted_close_price):\n",
    "    print(prediction[0], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be695b28-2791-4fd1-b0eb-86104cf5d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b95835f9-f52a-4c00-8b38-553da26c9016/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b95835f9-f52a-4c00-8b38-553da26c9016/assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('model_map_headlines.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer,f)\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb0f45-b7f7-49f0-9a58-e5a27060ba31",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab80799-8d0d-4f73-a013-5d706d6edc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>MAP_3</th>\n",
       "      <th>MAP_9</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Price_Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159.669998</td>\n",
       "      <td>158.269999</td>\n",
       "      <td>157.346666</td>\n",
       "      <td>A. Schulman Enters Oversold Territory - Tale o...</td>\n",
       "      <td>1.889999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160.380005</td>\n",
       "      <td>159.563334</td>\n",
       "      <td>157.842222</td>\n",
       "      <td>Zacks Releases Four Powerful ''Buy'' Stocks: I...</td>\n",
       "      <td>0.710007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162.009995</td>\n",
       "      <td>162.655557</td>\n",
       "      <td>161.775558</td>\n",
       "      <td>Waters Outperforms Q4 Estimate - Analyst Blog</td>\n",
       "      <td>1.629990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167.350006</td>\n",
       "      <td>167.820002</td>\n",
       "      <td>167.544444</td>\n",
       "      <td>Agilent Reports In Line - Analyst Blog</td>\n",
       "      <td>5.340011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>166.610001</td>\n",
       "      <td>168.130000</td>\n",
       "      <td>170.912221</td>\n",
       "      <td>Waters Reiterates at Neutral - Analyst Blog</td>\n",
       "      <td>-0.740005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close       MAP_3       MAP_9  \\\n",
       "1  159.669998  158.269999  157.346666   \n",
       "2  160.380005  159.563334  157.842222   \n",
       "3  162.009995  162.655557  161.775558   \n",
       "4  167.350006  167.820002  167.544444   \n",
       "5  166.610001  168.130000  170.912221   \n",
       "\n",
       "                                            Headline  Price_Diff  \n",
       "1  A. Schulman Enters Oversold Territory - Tale o...    1.889999  \n",
       "2  Zacks Releases Four Powerful ''Buy'' Stocks: I...    0.710007  \n",
       "3      Waters Outperforms Q4 Estimate - Analyst Blog    1.629990  \n",
       "4             Agilent Reports In Line - Analyst Blog    5.340011  \n",
       "5        Waters Reiterates at Neutral - Analyst Blog   -0.740005  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df = map_df.copy()\n",
    "sent_df['Price_Diff'] = sent_df['Close'].diff()\n",
    "sent_df.drop(index=sent_df.index[0], inplace=True)\n",
    "sent_df.drop(['Date'], axis=1, inplace=True)\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bbc4ac9-8c9b-443e-bac7-d791e6266caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load FinBERT model and tokenizer\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # Pretrained FinBERT model for financial sentiment\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Function to compute sentiment score\n",
    "def compute_sentiment_score(texts):\n",
    "    \"\"\"\n",
    "    Compute sentiment score for a list of financial texts using FinBERT.\n",
    "    :param texts: List of strings containing financial news or headlines.\n",
    "    :return: List of sentiment scores in the range (-1, 1).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for text in texts:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Get probabilities for each sentiment class\n",
    "        logits = outputs.logits.squeeze().tolist()\n",
    "        # Calculate sentiment score: positive logit - negative logit\n",
    "        score = logits[2] - logits[0]\n",
    "        scores.append(score)\n",
    "    return np.array(scores)\n",
    "\n",
    "# Assuming `news_df` has a column 'headline' containing the financial news headlines\n",
    "scores = compute_sentiment_score(sent_df[\"Headline\"].tolist())\n",
    "sent_df['Sentiment_Scores'] = (scores-np.mean(scores))/np.std(scores)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(sent_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5a704-c213-409a-855a-3eab3ad5c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sent_df.drop(columns=['Close','Headline','Price_Diff'])\n",
    "Y = sent_df['Close']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)\n",
    "print(\"Percentage error:\", np.mean(np.abs(Y_test-Y_pred/Y_test))/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f361ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8098395d608241b79bab90bb9e216c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7583aa73041e403aaf36cdac3e2b8f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1111f63124f4edcaaffbbb90d0e9dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1534371376037598\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# user_input = request.POST.get('user_input')\n",
    "user_input=\"Gold Hits Record High as Analysts Predict Continued Strength Amid Economic Uncertainty\"\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # Pretrained FinBERT model for financial sentiment\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(user_input, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits.squeeze().tolist()\n",
    "\n",
    "score = logits[2] - logits[0]\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
